{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-16T13:14:21.455540Z",
     "start_time": "2025-10-16T13:14:21.435035Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "class AnimalDataset(Dataset):\n",
    "    def __init__(self, img_dir, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "\n",
    "        # 遍历文件夹下的所有图片\n",
    "        for fname in os.listdir(img_dir):\n",
    "            if fname.endswith('.jpg') or fname.endswith('.png'):\n",
    "                # 根据文件名前缀提取标签\n",
    "                if fname.startswith('cat'):\n",
    "                    label = 0\n",
    "                elif fname.startswith('dog'):\n",
    "                    label = 1\n",
    "                else:\n",
    "                    continue\n",
    "                self.data.append((fname, label))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fname, label = self.data[idx] # 获取图片名和标签\n",
    "        img_path = os.path.join(self.img_dir, fname) # 构建图片路径\n",
    "        image = Image.open(img_path).convert('RGB')  # 转为RGB # 读取图片\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            image = transforms.Compose([\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.ToTensor()\n",
    "            ])(image)\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, img_dir, csv_path, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # 读取CSV文件\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "        # 保存每张图片的id和label\n",
    "        self.data = [(str(row['id']) + '.jpg', int(row['label'])) for _, row in df.iterrows()]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fname, label = self.data[idx]\n",
    "        img_path = os.path.join(self.img_dir, fname)\n",
    "        image = Image.open(img_path).convert('RGB')  # RGB彩色图像\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            image = transforms.Compose([\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.ToTensor()\n",
    "            ])(image)\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T13:14:22.213080Z",
     "start_time": "2025-10-16T13:14:21.833292Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 定义图像预处理\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# 加载数据集\n",
    "train_dataset = AnimalDataset('data/dog_and_cat/train', transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataset = TestDataset('data/dog_and_cat/test', csv_path='data/dog_and_cat/sampleSubmission.csv', transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# # 取一批数据看看\n",
    "# images, labels = next(iter(train_loader))\n",
    "# print(images)   # torch.Size([32, 3, 28, 28])\n",
    "# print(labels[:10])\n",
    "\n",
    "# 取一批数据看看\n",
    "images, labels = next(iter(test_loader))\n",
    "print(images)\n",
    "print(labels[:10])"
   ],
   "id": "f95dc0447629f452",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.1529, 0.1451, 0.1569,  ..., 0.3333, 0.2784, 0.1882],\n",
      "          [0.1647, 0.1490, 0.1373,  ..., 0.3333, 0.2745, 0.1882],\n",
      "          [0.1529, 0.1255, 0.1373,  ..., 0.3451, 0.2902, 0.2000],\n",
      "          ...,\n",
      "          [0.6824, 0.7098, 0.7176,  ..., 0.5373, 0.5373, 0.5098],\n",
      "          [0.6431, 0.6941, 0.7216,  ..., 0.4667, 0.3804, 0.3882],\n",
      "          [0.6667, 0.6784, 0.7098,  ..., 0.6392, 0.4667, 0.4157]],\n",
      "\n",
      "         [[0.2588, 0.2471, 0.2588,  ..., 0.4627, 0.4157, 0.3333],\n",
      "          [0.2706, 0.2510, 0.2392,  ..., 0.4549, 0.4078, 0.3255],\n",
      "          [0.2588, 0.2275, 0.2392,  ..., 0.4510, 0.4000, 0.3216],\n",
      "          ...,\n",
      "          [0.6980, 0.7255, 0.7333,  ..., 0.5216, 0.5137, 0.4824],\n",
      "          [0.6588, 0.7059, 0.7373,  ..., 0.4588, 0.3686, 0.3725],\n",
      "          [0.6784, 0.6902, 0.7216,  ..., 0.6078, 0.4353, 0.3804]],\n",
      "\n",
      "         [[0.3765, 0.3804, 0.4000,  ..., 0.8392, 0.7804, 0.6784],\n",
      "          [0.3882, 0.3843, 0.3804,  ..., 0.8353, 0.7725, 0.6745],\n",
      "          [0.3765, 0.3608, 0.3804,  ..., 0.8392, 0.7804, 0.6824],\n",
      "          ...,\n",
      "          [0.7412, 0.7725, 0.7804,  ..., 0.4471, 0.4314, 0.3922],\n",
      "          [0.6902, 0.7373, 0.7804,  ..., 0.4000, 0.2902, 0.2824],\n",
      "          [0.6863, 0.7059, 0.7608,  ..., 0.5804, 0.3686, 0.2941]]],\n",
      "\n",
      "\n",
      "        [[[0.2471, 0.2471, 0.2431,  ..., 0.3647, 0.4549, 0.5020],\n",
      "          [0.2667, 0.2627, 0.2588,  ..., 0.3686, 0.4314, 0.4667],\n",
      "          [0.2667, 0.2627, 0.2510,  ..., 0.3569, 0.4078, 0.4431],\n",
      "          ...,\n",
      "          [0.9216, 0.9137, 0.9333,  ..., 0.2510, 0.2549, 0.2588],\n",
      "          [0.9294, 0.9255, 0.9490,  ..., 0.2471, 0.2235, 0.2510],\n",
      "          [0.9373, 0.9373, 0.9686,  ..., 0.2549, 0.2314, 0.2627]],\n",
      "\n",
      "         [[0.1098, 0.1098, 0.1059,  ..., 0.2039, 0.2784, 0.3137],\n",
      "          [0.1294, 0.1255, 0.1176,  ..., 0.2039, 0.2510, 0.2745],\n",
      "          [0.1255, 0.1216, 0.1098,  ..., 0.1843, 0.2235, 0.2471],\n",
      "          ...,\n",
      "          [0.8039, 0.7725, 0.7647,  ..., 0.1490, 0.1490, 0.1529],\n",
      "          [0.8078, 0.7843, 0.7765,  ..., 0.1333, 0.1098, 0.1373],\n",
      "          [0.8157, 0.7922, 0.7961,  ..., 0.1373, 0.1176, 0.1451]],\n",
      "\n",
      "         [[0.1020, 0.1020, 0.0980,  ..., 0.1451, 0.2000, 0.2235],\n",
      "          [0.1255, 0.1216, 0.1137,  ..., 0.1451, 0.1765, 0.1843],\n",
      "          [0.1294, 0.1255, 0.1137,  ..., 0.1333, 0.1490, 0.1569],\n",
      "          ...,\n",
      "          [0.6510, 0.6235, 0.6275,  ..., 0.1569, 0.1569, 0.1569],\n",
      "          [0.6588, 0.6392, 0.6471,  ..., 0.1412, 0.1176, 0.1451],\n",
      "          [0.6706, 0.6588, 0.6706,  ..., 0.1412, 0.1216, 0.1529]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          ...,\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
      "\n",
      "         [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          ...,\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
      "\n",
      "         [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          ...,\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0.2353, 0.2353, 0.2314,  ..., 0.6157, 0.6235, 0.6275],\n",
      "          [0.2353, 0.2353, 0.2314,  ..., 0.6157, 0.6235, 0.6314],\n",
      "          [0.2353, 0.2353, 0.2314,  ..., 0.6157, 0.6235, 0.6314],\n",
      "          ...,\n",
      "          [0.1686, 0.1647, 0.1686,  ..., 0.6980, 0.6275, 0.6667],\n",
      "          [0.1725, 0.1647, 0.1490,  ..., 0.9176, 0.9020, 0.8941],\n",
      "          [0.1882, 0.1804, 0.1490,  ..., 0.9412, 0.9412, 0.9490]],\n",
      "\n",
      "         [[0.2196, 0.2196, 0.2157,  ..., 0.6980, 0.6824, 0.6863],\n",
      "          [0.2196, 0.2196, 0.2157,  ..., 0.6941, 0.6824, 0.6902],\n",
      "          [0.2196, 0.2196, 0.2157,  ..., 0.6902, 0.6824, 0.6902],\n",
      "          ...,\n",
      "          [0.1059, 0.1098, 0.1137,  ..., 0.7255, 0.6588, 0.7020],\n",
      "          [0.1176, 0.1176, 0.1059,  ..., 0.9333, 0.9176, 0.9098],\n",
      "          [0.1451, 0.1412, 0.1176,  ..., 0.9490, 0.9451, 0.9529]],\n",
      "\n",
      "         [[0.2863, 0.2863, 0.2824,  ..., 0.8275, 0.8039, 0.8078],\n",
      "          [0.2863, 0.2863, 0.2824,  ..., 0.8235, 0.8039, 0.8118],\n",
      "          [0.2863, 0.2863, 0.2824,  ..., 0.8196, 0.8039, 0.8118],\n",
      "          ...,\n",
      "          [0.1882, 0.1922, 0.1922,  ..., 0.7608, 0.6980, 0.7373],\n",
      "          [0.1804, 0.1882, 0.1765,  ..., 0.9412, 0.9373, 0.9294],\n",
      "          [0.2000, 0.2039, 0.1843,  ..., 0.9490, 0.9529, 0.9608]]],\n",
      "\n",
      "\n",
      "        [[[0.2706, 0.0706, 0.0745,  ..., 0.3922, 0.3569, 0.3451],\n",
      "          [0.3216, 0.0980, 0.0824,  ..., 0.2863, 0.2353, 0.2471],\n",
      "          [0.3765, 0.1216, 0.0784,  ..., 0.2627, 0.2627, 0.2706],\n",
      "          ...,\n",
      "          [0.2078, 0.2118, 0.2235,  ..., 0.4706, 0.4392, 0.4314],\n",
      "          [0.3294, 0.2745, 0.3020,  ..., 0.4471, 0.4000, 0.3961],\n",
      "          [0.4235, 0.3333, 0.3804,  ..., 0.4784, 0.3961, 0.3020]],\n",
      "\n",
      "         [[0.2471, 0.0471, 0.0510,  ..., 0.3647, 0.3176, 0.3059],\n",
      "          [0.2980, 0.0745, 0.0588,  ..., 0.2549, 0.1961, 0.2118],\n",
      "          [0.3529, 0.0980, 0.0549,  ..., 0.2275, 0.2235, 0.2314],\n",
      "          ...,\n",
      "          [0.2039, 0.2078, 0.2196,  ..., 0.4902, 0.4588, 0.4510],\n",
      "          [0.3255, 0.2706, 0.2980,  ..., 0.4706, 0.4235, 0.4235],\n",
      "          [0.4196, 0.3294, 0.3765,  ..., 0.5098, 0.4275, 0.3333]],\n",
      "\n",
      "         [[0.2627, 0.0627, 0.0588,  ..., 0.3922, 0.3490, 0.3373],\n",
      "          [0.3137, 0.0902, 0.0667,  ..., 0.2745, 0.2196, 0.2314],\n",
      "          [0.3686, 0.1137, 0.0627,  ..., 0.2431, 0.2392, 0.2471],\n",
      "          ...,\n",
      "          [0.2275, 0.2314, 0.2431,  ..., 0.5373, 0.5059, 0.4980],\n",
      "          [0.3490, 0.2941, 0.3216,  ..., 0.5176, 0.4706, 0.4706],\n",
      "          [0.4431, 0.3529, 0.4000,  ..., 0.5529, 0.4706, 0.3765]]],\n",
      "\n",
      "\n",
      "        [[[0.5216, 0.5137, 0.4941,  ..., 0.0863, 0.0863, 0.0784],\n",
      "          [0.5176, 0.5020, 0.4706,  ..., 0.0863, 0.0863, 0.0784],\n",
      "          [0.4784, 0.4863, 0.4588,  ..., 0.0863, 0.0863, 0.0824],\n",
      "          ...,\n",
      "          [0.0275, 0.0275, 0.0275,  ..., 0.4627, 0.4588, 0.4510],\n",
      "          [0.0235, 0.0235, 0.0235,  ..., 0.4549, 0.4471, 0.4431],\n",
      "          [0.0235, 0.0235, 0.0235,  ..., 0.4431, 0.4353, 0.4275]],\n",
      "\n",
      "         [[0.3176, 0.3412, 0.3804,  ..., 0.1569, 0.1686, 0.1647],\n",
      "          [0.3373, 0.3451, 0.3686,  ..., 0.1608, 0.1686, 0.1686],\n",
      "          [0.3333, 0.3686, 0.3882,  ..., 0.1608, 0.1686, 0.1686],\n",
      "          ...,\n",
      "          [0.0902, 0.0902, 0.0902,  ..., 0.3961, 0.3922, 0.3843],\n",
      "          [0.0863, 0.0863, 0.0863,  ..., 0.3882, 0.3804, 0.3765],\n",
      "          [0.0863, 0.0863, 0.0863,  ..., 0.3765, 0.3686, 0.3608]],\n",
      "\n",
      "         [[0.1686, 0.2000, 0.2471,  ..., 0.2118, 0.2235, 0.2196],\n",
      "          [0.1961, 0.2157, 0.2588,  ..., 0.2118, 0.2235, 0.2235],\n",
      "          [0.2078, 0.2510, 0.3059,  ..., 0.2118, 0.2235, 0.2235],\n",
      "          ...,\n",
      "          [0.1529, 0.1529, 0.1529,  ..., 0.3255, 0.3216, 0.3137],\n",
      "          [0.1490, 0.1490, 0.1490,  ..., 0.3176, 0.3098, 0.3059],\n",
      "          [0.1490, 0.1490, 0.1490,  ..., 0.3059, 0.2980, 0.2902]]]])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T13:24:31.581507Z",
     "start_time": "2025-10-16T13:14:22.854755Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from utils.accuracy import evaluate_accuracy\n",
    "import torch\n",
    "from torch import nn\n",
    "from utils.init import init_weights\n",
    "from models.alexnet import AlexNet\n",
    "\n",
    "# 定义网络\n",
    "net = AlexNet()\n",
    "net.apply(init_weights)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "# 训练\n",
    "epochs = 10\n",
    "train_losses, train_accs, test_accs = [], [], []\n",
    "for epoch in range(1, epochs+1):\n",
    "    net.train()\n",
    "    total_loss, total_acc, total_count = 0, 0, 0\n",
    "    for X, y in train_loader:\n",
    "        y_hat = net(X)\n",
    "        l = loss(y_hat, y)\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        total_loss += l.item() * y.numel()\n",
    "        total_acc += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "        total_count += y.numel()\n",
    "\n",
    "    train_acc = total_acc / total_count\n",
    "    train_loss = total_loss / total_count\n",
    "    test_acc = evaluate_accuracy(net, test_loader)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    test_accs.append(test_acc)\n",
    "    print(f'Epoch {epoch}, Loss {train_loss:.4f}, Acc {train_acc:.4f}, Test Acc {test_acc:.4f}')\n",
    "\n",
    "plt.figure(figsize=[12, 5])\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, epochs+1), train_losses, 'o-', label='Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, epochs+1), train_accs, 'o-', label='Training Accuracy')\n",
    "plt.plot(range(1, epochs+1), test_accs, 'o-', label='Test Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ],
   "id": "b91b144698294f9c",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[28], line 36\u001B[0m\n\u001B[0;32m     34\u001B[0m train_acc \u001B[38;5;241m=\u001B[39m total_acc \u001B[38;5;241m/\u001B[39m total_count\n\u001B[0;32m     35\u001B[0m train_loss \u001B[38;5;241m=\u001B[39m total_loss \u001B[38;5;241m/\u001B[39m total_count\n\u001B[1;32m---> 36\u001B[0m test_acc \u001B[38;5;241m=\u001B[39m \u001B[43mevaluate_accuracy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnet\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_loader\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     38\u001B[0m train_losses\u001B[38;5;241m.\u001B[39mappend(train_loss)\n\u001B[0;32m     39\u001B[0m train_accs\u001B[38;5;241m.\u001B[39mappend(train_acc)\n",
      "File \u001B[1;32mF:\\base\\utils\\accuracy.py:17\u001B[0m, in \u001B[0;36mevaluate_accuracy\u001B[1;34m(net, data_iter)\u001B[0m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m     16\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m x, y \u001B[38;5;129;01min\u001B[39;00m data_iter:\n\u001B[1;32m---> 17\u001B[0m         acc \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m accuracy_count(\u001B[43mnet\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m, y)\n\u001B[0;32m     18\u001B[0m         total \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m y\u001B[38;5;241m.\u001B[39mnumel()\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m acc \u001B[38;5;241m/\u001B[39m total\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\d2l\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mF:\\base\\models\\alexnet.py:31\u001B[0m, in \u001B[0;36mAlexNet.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     30\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m---> 31\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     32\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\d2l\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\d2l\\lib\\site-packages\\torch\\nn\\modules\\container.py:139\u001B[0m, in \u001B[0;36mSequential.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    137\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[0;32m    138\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[1;32m--> 139\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    140\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\d2l\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\d2l\\lib\\site-packages\\torch\\nn\\modules\\conv.py:457\u001B[0m, in \u001B[0;36mConv2d.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    456\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 457\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\d2l\\lib\\site-packages\\torch\\nn\\modules\\conv.py:453\u001B[0m, in \u001B[0;36mConv2d._conv_forward\u001B[1;34m(self, input, weight, bias)\u001B[0m\n\u001B[0;32m    449\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mzeros\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m    450\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv2d(F\u001B[38;5;241m.\u001B[39mpad(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode),\n\u001B[0;32m    451\u001B[0m                     weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride,\n\u001B[0;32m    452\u001B[0m                     _pair(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups)\n\u001B[1;32m--> 453\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    454\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "53abdd0f85682a67"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
